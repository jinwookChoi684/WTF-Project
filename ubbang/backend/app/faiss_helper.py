import os
import uuid
from typing import List, Dict, Optional
from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.text_splitter import RecursiveCharacterTextSplitter

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
load_dotenv()
embedding_model = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

# í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=30
)

# âœ… 1. FAISS ì €ì¥ í•¨ìˆ˜ (pkë³„ë¡œ ì €ì¥)
def save_to_faiss(pk: str, messages: List[Dict[str, str]]):
    """
    ì£¼ì–´ì§„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ FAISSì— ì €ì¥.
    ê° ë©”ì‹œì§€ëŠ” {"role": "user"/"assistant", "content": "..."} í˜•íƒœ
    """
    texts = []
    metadatas = []

    for msg in messages:
        role_prefix = "[ìœ ì €]" if msg["role"] == "user" else "[ìš°ë¹µ]"
        text = f"{role_prefix} {msg['content']}"
        # print(f"ğŸ§© ì›ë³¸ í…ìŠ¤íŠ¸: {text!r}")  # ë””ë²„ê·¸ ë¡œê·¸
        chunks = text_splitter.split_text(text)
        # print(f"ğŸ§© ë¶„í• ëœ ì¡°ê° ê°œìˆ˜: {len(chunks)}")  # ë””ë²„ê·¸ ë¡œê·¸

        for chunk in chunks:
            texts.append(chunk)
            metadatas.append({
                "pk": pk,
                "message_id": str(uuid.uuid4()),
                "role": msg["role"]
            })
    if not texts:
        print(f"âš ï¸ ì €ì¥í•  ìœ íš¨ í…ìŠ¤íŠ¸ ì—†ìŒ. FAISS ì €ì¥ ìƒëµ (pk: {pk})")
        return

    faiss_path = f"vectorstore/faiss_index/{pk}"
    os.makedirs(faiss_path, exist_ok=True)

    if os.path.exists(os.path.join(faiss_path, "index.faiss")):
        db = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)
        db.add_texts(texts, metadatas=metadatas)
    else:
        db = FAISS.from_texts(texts, embedding_model, metadatas=metadatas)

    db.save_local(faiss_path)
    print(f"âœ… FAISS ì €ì¥ ì™„ë£Œ ({pk}): {len(texts)}ê°œ ì¡°ê°")


# âœ… 2. FAISS ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
def search_from_faiss(pk: str, query: str, k: int = 3) -> List[str]:
    """
    ìœ ì € ì¿¼ë¦¬ì— ëŒ€í•´ FAISSì—ì„œ ìœ ì‚¬í•œ ë©”ì‹œì§€ top-k ê²€ìƒ‰
    """
    faiss_path = f"vectorstore/faiss_index/{pk}"
    if not os.path.exists(os.path.join(faiss_path, "index.faiss")):
        print(f"â—FAISS ì¸ë±ìŠ¤ ì—†ìŒ: {faiss_path}")
        return []

    db = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)
    docs = db.similarity_search(query, k=k)
    return [doc.page_content for doc in docs]


# âœ… 3. FAISS ë¡œë“œ í•¨ìˆ˜ (ì˜µì…˜)
def load_faiss_index(pk: str) -> Optional[FAISS]:
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œ. ì—†ìœ¼ë©´ None ë°˜í™˜
    """
    try:
        faiss_path = f"vectorstore/faiss_index/{pk}"
        if not os.path.exists(os.path.join(faiss_path, "index.faiss")):
            return None
        return FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"[FAISS ë¡œë“œ ì‹¤íŒ¨] {e}")
        return None
