



## ****** ë³¸ì§ˆì ìœ¼ë¡œ "ì‘ë‹µìƒì„±(completion)"ì— ì§‘ì¤‘í•˜ëŠ” pyíŒŒì¼ ì…ë‹ˆë‹¤ *****
## GPT ì‘ë‹µìƒì„±






## ì±—ë´‡ ì‘ë‹µì— ê´€í•œ ì‘ë‹µí•¨ìˆ˜ë“¤ì„ ì´ íŒŒì¼ë¡œ ë‹¤ ëº´ë†“ì„ê±°ì„
## ê·¸ë˜ì•¼ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ì— ìˆì–´ì„œ ìš©ì´í•˜ê¸°ë•œì‹œ
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨

# ---------------------------------------------------------------------
# 6/30 ì—…ë°ì´íŠ¸ ì‚¬í•­
# BufferMemory ê´€ë ¨ ì½”ë“œ ìˆ˜ì •
# get_cahtbot_response()ë¥¼ memoryê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±
# chat.pyë„ ë³€ê²½
# memory_store.py ìƒˆë¡œ ìƒì„± : LangChain ì´ˆê¸°í™” ë° memory ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
# get_chatbot_response() í•¨ìˆ˜ê°€ history ëŒ€ì‹  memoryë¥¼ ë°›ì•„ì„œ ì‘ë‹µ ìƒì„±
# LangChainì˜ ConversationChain ì‚¬ìš©
# ------------------------------------------------------------------------


# app/openai_helper.py

## ì±—ë´‡ ì‘ë‹µì— ê´€í•œ ì‘ë‹µí•¨ìˆ˜ë“¤ì„ ì´ íŒŒì¼ë¡œ ë‹¤ ë¹¼ë†“ìŒ
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨

from openai import OpenAI
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
llm = ChatOpenAI(model="gpt-4o")




# ğŸ¯ LangChain ê¸°ë°˜ ì‘ë‹µ memory (ìœ ì €ë³„ë¡œ ë¶„ë¦¬)
user_memory_store = {}


def get_user_memory(pk: str) -> ConversationBufferMemory:
    if pk not in user_memory_store:
        user_memory_store[pk] = ConversationBufferMemory(return_messages=True)
    return user_memory_store[pk]


async def get_chatbot_response(pk: str, user_input: str, system_prompt: str, memory) -> str:
    try:
        memory = get_user_memory(pk)

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        chain = LLMChain(llm=llm, memory=memory, prompt=prompt)
        result = chain.run({"input": user_input})
        return result

    except Exception as e:
        print(f"[ERROR] get_chatbot_response ì‹¤íŒ¨: {e}")
        return "âš ï¸ ì±—ë´‡ ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´."


# ğŸ¯ ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (ê°œì¸ê¸°ë¡ / ì™¸ë¶€ì •ë³´ / ì¼ë°˜ëŒ€í™”)
def detect_query_type(message: str) -> str:
    prompt = f"""
    ë‹¤ìŒ ìœ ì €ì˜ ë°œí™”ë¥¼ ì½ê³ , ì–´ë–¤ ì‘ë‹µ ë°©ì‹ì´ ì ì ˆí•œì§€ í•˜ë‚˜ë§Œ ê³¨ë¼ì¤˜.
    - ê°œì¸ê¸°ë¡ê²€ìƒ‰
    - ì™¸ë¶€ì •ë³´ê²€ìƒ‰
    - ì¼ë°˜ëŒ€í™”

    ìœ ì € ë°œí™”: "{message}"
    ì ì ˆí•œ ë°©ì‹:"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )
    return response.choices[0].message.content.strip()

# âœ… ë¬¸ë§¥ì— ë”°ë¼ ë‚ ì”¨/ì‹œê°„ ì‘ë‹µì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
def should_trigger_contextual_info(user_input: str, info_type: str) -> bool:
    """
    GPTì—ê²Œ í˜„ì¬ ë°œí™”ì—ì„œ ì‹¤ì œ ì •ë³´ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ê²Œ í•¨
    info_type: "ë‚ ì”¨" or "ì‹œê°„"
    """
    question = f"""
ì‚¬ìš©ìì˜ ë°œí™”ê°€ ì•„ë˜ì™€ ê°™ì„ ë•Œ, {info_type} ì •ë³´ë¥¼ ì‹¤ì œë¡œ ì œê³µí•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•´ì¤˜.
- "ì˜ˆ": ìœ ì €ê°€ ì§€ê¸ˆ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš° (ì˜ˆ: "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?")
- "ì•„ë‹ˆì˜¤": ë‹¨ìˆœ ì–¸ê¸‰ì´ê±°ë‚˜ ê³¼ê±°/ë¹„ìœ ì ì¸ í‘œí˜„ (ì˜ˆ: "ì–´ì œ ë‚ ì”¨ ì§„ì§œ ë³„ë¡œì˜€ì§€")

ë°˜ë“œì‹œ "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë§Œ ëŒ€ë‹µí•´ì¤˜.

ë°œí™”: "{user_input}"
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": question}],
            temperature=0.0
        )
        answer = response.choices[0].message.content.strip()
        return "ì˜ˆ" in answer
    except Exception as e:
        print(f"[GPT íŒë‹¨ ì‹¤íŒ¨: {e}]")
        return False