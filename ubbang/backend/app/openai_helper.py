
## GPT ì‘ë‹µìƒì„±
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨


## ì±—ë´‡ ì‘ë‹µì— ê´€í•œ ì‘ë‹µí•¨ìˆ˜ë“¤ì„ ì´ íŒŒì¼ë¡œ ë‹¤ ëº´ë†“ì„ê±°ì„
## ê·¸ë˜ì•¼ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ì— ìˆì–´ì„œ ìš©ì´í•˜ê¸°ë•œì‹œ
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨

# ---------------------------------------------------------------------
# 6/30 ì—…ë°ì´íŠ¸ ì‚¬í•­
# BufferMemory ê´€ë ¨ ì½”ë“œ ìˆ˜ì •
# get_cahtbot_response()ë¥¼ memoryê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±
# chat.pyë„ ë³€ê²½
# memory_store.py ìƒˆë¡œ ìƒì„± : LangChain ì´ˆê¸°í™” ë° memory ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
# get_chatbot_response() í•¨ìˆ˜ê°€ history ëŒ€ì‹  memoryë¥¼ ë°›ì•„ì„œ ì‘ë‹µ ìƒì„±
# LangChainì˜ ConversationChain ì‚¬ìš©
# ------------------------------------------------------------------------


# app/openai_helper.py
from openai import OpenAI
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# --------------------------------------------------------------------------------

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
llm = ChatOpenAI(model="gpt-4o")

# -------------------------------------------------------------------------------


# ğŸ¯ LangChain ê¸°ë°˜ ì‘ë‹µ memory (ìœ ì €ë³„ë¡œ ë¶„ë¦¬)
user_memory_store = {}


def get_user_memory(pk: str) -> ConversationBufferMemory:
    if pk not in user_memory_store:
        user_memory_store[pk] = ConversationBufferMemory(return_messages=True)
    return user_memory_store[pk]



# --------------------------------------------------------------------------------
# (ìœ ì € ì¸í’‹+ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸+ë²„í¼ë©”ëª¨ë¦¬)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µìƒì„±í•˜ëŠ” í•¨ìˆ˜
async def get_chatbot_response(
    user_input: str,
    system_prompt: str,
    memory: ConversationBufferMemory) -> str:


    # LangChain message ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    # 1. ëŒ€í™” ì´ë ¥ ì •ë¦¬
    chat_history = memory.chat_memory.messages
    messages = [{"role": "system", "content": system_prompt}]

    for msg in chat_history:
        if msg.type == "human":
            messages.append({"role": "user", "content": msg.content})
        elif msg.type == "ai":
            messages.append({"role": "assistant", "content": msg.content})
    
    # 2. ìµœì‹  ìœ ì € ì…ë ¥ ì¶”ê°€
    messages.append({"role": "user", "content": user_input})
    
    # 3. ì§€í”¼í‹° ì‘ë‹µ ìƒì„±
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"[GPT ì‘ë‹µ ì‹¤íŒ¨] {e}")
        return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì¤˜!"

# ---------------------------------------------------------------------------------

# ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (ê°œì¸ê¸°ë¡ / ì™¸ë¶€ì •ë³´ / ì¼ë°˜ëŒ€í™”)
def detect_query_type(message: str) -> str:
    prompt = f"""
    ë‹¤ìŒ ìœ ì €ì˜ ë°œí™”ë¥¼ ì½ê³ , ì–´ë–¤ ì‘ë‹µ ë°©ì‹ì´ ì ì ˆí•œì§€ í•˜ë‚˜ë§Œ ê³¨ë¼ì¤˜.
    - ê°œì¸ê¸°ë¡ê²€ìƒ‰
    - ì™¸ë¶€ì •ë³´ê²€ìƒ‰
    - ì¼ë°˜ëŒ€í™”

    ìœ ì € ë°œí™”: "{message}"
    ì ì ˆí•œ ë°©ì‹:"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )
    return response.choices[0].message.content.strip()

