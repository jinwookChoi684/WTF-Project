## GPT ì‘ë‹µìƒì„±
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨

# ---------------------------------------------------------------------
# 6/30 ì‚¬í•­
# BufferMemory ê´€ë ¨ ì½”ë“œ ìˆ˜ì •
# chat.pyì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ë“¤ë§Œ ì •ë¦¬
# ë¶ˆí•„ìš”í•œ í•¨ìˆ˜ ì œê±° ë° ì¸ì êµ¬ì¡° í†µì¼
# ------------------------------------------------------------------------

# app/openai_helper.py
from openai import OpenAI
import os
from dotenv import load_dotenv

from .BasePrompt_builder import BasePromptBuilder
from .dynamo_utils import get_recent_messages_from_dynamo

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from .prompt_assembler import assemble_system_prompt

from typing import List
from openai import AsyncOpenAI

# --------------------------------------------------------------------------------

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

# -------------------------------------------------------------------------------

# ğŸ¯ LangChain ê¸°ë°˜ ì‘ë‹µ memory (ìœ ì €ë³„ë¡œ ë¶„ë¦¬)
user_memory_store = {}


def get_user_memory(pk: str) -> ConversationBufferMemory:
    """
    ìœ ì €ë³„ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸° (DynamoDBì—ì„œ ìµœê·¼ ë©”ì‹œì§€ ë³µì›)
    """
    if pk in user_memory_store:
        return user_memory_store[pk]

    memory = ConversationBufferMemory(return_messages=True)

    # ğŸ’¡ DynamoDBì—ì„œ ìµœê·¼ ë©”ì‹œì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    recent = get_recent_messages_from_dynamo(pk, limit=10)
    for msg in recent:
        if msg["role"] == "user":
            memory.chat_memory.add_user_message(msg["content"])
        elif msg["role"] == "assistant":
            memory.chat_memory.add_ai_message(msg["content"])

    user_memory_store[pk] = memory
    return memory


# --------------------------------------------------------------------------------

# ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (ê°œì¸ê¸°ë¡ / ì™¸ë¶€ì •ë³´ / ì¼ë°˜ëŒ€í™”)
def detect_query_type(message: str) -> str:
    """
    ìœ ì €ë©”ì‹œì§€ì˜ ì¿¼ë¦¬ íƒ€ì…ì„ ê°ì§€
    """
    prompt = f"""
    ë‹¤ìŒ ìœ ì €ì˜ ë°œí™”ë¥¼ ì½ê³ , ì–´ë–¤ ì‘ë‹µ ë°©ì‹ì´ ì ì ˆí•œì§€ í•˜ë‚˜ë§Œ ê³¨ë¼ì¤˜.
    - ê°œì¸ê¸°ë¡ê²€ìƒ‰
    - ì™¸ë¶€ì •ë³´ê²€ìƒ‰
    - ì¼ë°˜ëŒ€í™”

    ìœ ì € ë°œí™”: "{message}"
    ì ì ˆí•œ ë°©ì‹:"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
    )
    return response.choices[0].message.content.strip()


# --------------------------------------------------------------------------------

# âœ… ë²¡í„° ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ (vector vs memory)
def should_use_vector_search(user_input: str) -> bool:
    """
    ì‚¬ìš©ì ì…ë ¥ì´ ê³¼ê±° ëŒ€í™” ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨
    """
    try:
        prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ê³¼ê±° ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•´ì•¼ í•  ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì¤˜.

ì§ˆë¬¸: "{user_input}"

ë‹µë³€ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ í•´ì¤˜:
- vector â†’ ê³¼ê±° ëŒ€í™” ê²€ìƒ‰ í•„ìš”
- memory â†’ í˜„ì¬ ëŒ€í™” íë¦„ì— ì‘ë‹µ

ì •ë‹µ:
"""

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer = response.choices[0].message.content.strip().lower()
        return "vector" in answer

    except Exception as e:
        print(f"[ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨] {e}")
        return False


# --------------------------------------------------------------------------------

async def summarize_chunks(chunks: List[str]) -> str:
    """
    FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½
    """
    if not chunks:
        return ""
    summary_prompt = f"""
ë‹¤ìŒì€ ê³¼ê±°ì˜ ëŒ€í™” ê¸°ë¡ì´ì•¼. ì´ê±¸ ë³´ê³  í•µì‹¬ ë‚´ìš©ì„ í•œ ë‹¨ë½ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜. ë„ˆë¬´ ê¸¸ê²Œ ë§í•˜ì§€ ë§ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì¤˜.

---

{chr(10).join(chunks)}

---
ìš”ì•½:"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"[ìš”ì•½ ì‹¤íŒ¨] {e}")
        return ""


# --------------------------------------------------------------------------------

# -- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸° ----------------------------------
stream_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))


async def stream_gpt_response(system_prompt: str, memory: ConversationBufferMemory, user_input: str):
    """
    íŠ¸ë¦¬ë° GPT ì‘ë‹µ ìƒì„±ê¸°
    """
    try:
        messages = [
            {"role": "system", "content": system_prompt},
        ]
        # memoryë¥¼ chat formatìœ¼ë¡œ ì¶”ê°€
        for msg in memory.chat_memory.messages:
            role = "user" if msg.type == "human" else "assistant"
            messages.append({"role": role, "content": msg.content})
        messages.append({"role": "user", "content": user_input})

        # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
        response = await stream_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.7,
            stream=True,
        )

        async for chunk in response:
            delta = chunk.choices[0].delta
            if delta and delta.content:
                yield delta.content
    except Exception as e:
        print(f"[Stream ì‘ë‹µ ì˜¤ë¥˜] {e}")
        yield "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."
