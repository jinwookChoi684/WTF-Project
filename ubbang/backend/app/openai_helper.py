
## GPT ì‘ë‹µìƒì„±
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨


## ì±—ë´‡ ì‘ë‹µì— ê´€í•œ ì‘ë‹µí•¨ìˆ˜ë“¤ì„ ì´ íŒŒì¼ë¡œ ë‹¤ ëº´ë†“ì„ê±°ì„
## ê·¸ë˜ì•¼ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ì— ìˆì–´ì„œ ìš©ì´í•˜ê¸°ë•œì‹œ
## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‘ë‹µ, ì¿¼ë¦¬ íƒ€ì… ê°ì§€ í¬í•¨

# ---------------------------------------------------------------------
# 6/30 ì—…ë°ì´íŠ¸ ì‚¬í•­
# BufferMemory ê´€ë ¨ ì½”ë“œ ìˆ˜ì •
# get_cahtbot_response()ë¥¼ memoryê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±
# chat.pyë„ ë³€ê²½
# memory_store.py ìƒˆë¡œ ìƒì„± : LangChain ì´ˆê¸°í™” ë° memory ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
# get_chatbot_response() í•¨ìˆ˜ê°€ history ëŒ€ì‹  memoryë¥¼ ë°›ì•„ì„œ ì‘ë‹µ ìƒì„±
# LangChainì˜ ConversationChain ì‚¬ìš©
# ------------------------------------------------------------------------

# app/openai_helper.py
from openai import OpenAI
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate
# noinspection PyUnresolvedReferences
from app.BasePrompt_builder import BasePromptBuilder

# --------------------------------------------------------------------------------

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# -------------------------------------------------------------------------------


# ğŸ¯ LangChain ê¸°ë°˜ ì‘ë‹µ memory (ìœ ì €ë³„ë¡œ ë¶„ë¦¬)
user_memory_store = {}
def get_user_memory(pk: str) -> ConversationBufferMemory:
    if pk not in user_memory_store:
        user_memory_store[pk] = ConversationBufferMemory(return_messages=True)
    return user_memory_store[pk]



# --------------------------------------------------------------------------------
# (ìœ ì € ì¸í’‹+ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸+ë²„í¼ë©”ëª¨ë¦¬)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µìƒì„±í•˜ëŠ” í•¨ìˆ˜
# âœ… ë©”ì¸ ì‘ë‹µ í•¨ìˆ˜ (LangChain Memory + FAISS context í†µí•©)
async def get_chatbot_response(
        pk: str,
        user_input: str,
        system_prompt: str,
        memory: ConversationBufferMemory,
        faiss_context: str = None
):
    try:
        # âœ… FAISS contextê°€ ìˆìœ¼ë©´ system_promptì— í¬í•¨
        if faiss_context:
            system_prompt += f"\n\n# ì¶”ê°€ ì •ë³´:\në‹¤ìŒì€ ì´ì „ì— ìœ ì €ê°€ ë‚¨ê¸´ ì¤‘ìš”í•œ ë‚´ìš©ì´ì•¼. ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´ì¤˜:\n{faiss_context}"

        # âœ… LangChain prompt êµ¬ì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            *memory.chat_memory.messages,
            ("human", "{input}"),
        ])

        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            memory=memory,
            verbose=False,
        )

        result = await chain.arun(input=user_input)
        return result
    except Exception as e:
        print(f"[Chatbot ì‘ë‹µ ì‹¤íŒ¨] {e}")
        return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”."

# ---------------------------------------------------------------------------------

# ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (ê°œì¸ê¸°ë¡ / ì™¸ë¶€ì •ë³´ / ì¼ë°˜ëŒ€í™”)
def detect_query_type(message: str) -> str:
    prompt = f"""
    ë‹¤ìŒ ìœ ì €ì˜ ë°œí™”ë¥¼ ì½ê³ , ì–´ë–¤ ì‘ë‹µ ë°©ì‹ì´ ì ì ˆí•œì§€ í•˜ë‚˜ë§Œ ê³¨ë¼ì¤˜.
    - ê°œì¸ê¸°ë¡ê²€ìƒ‰
    - ì™¸ë¶€ì •ë³´ê²€ìƒ‰
    - ì¼ë°˜ëŒ€í™”

    ìœ ì € ë°œí™”: "{message}"
    ì ì ˆí•œ ë°©ì‹:"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì…ë ¥ëœ ë¬¸ì¥ì„ ì ì ˆí•œ ì‘ë‹µ ë°©ì‹ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¶„ë¥˜ê¸°ì•¼."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.0,
    )
    return response.choices[0].message.content.strip()


# âœ… FAISS ê¸°ë°˜ ê°œì¸ê¸°ë¡ ê²€ìƒ‰ + GPT ì‘ë‹µ ìƒì„± í•¨ìˆ˜
from .faiss_helper import search_from_faiss


async def get_rag_response(
    user_input: str,
    memory: ConversationBufferMemory,
    pk: str,
    gender: str,
    mode: str,
    age: int,
    tf: str
) -> str:
    # 1. ê²€ìƒ‰
    retrieved_chunks = search_from_faiss(pk, user_input, k=10)
    if not retrieved_chunks:
        return "ğŸ§  ê³¼ê±° ëŒ€í™” ì¤‘ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆì–´. ë‹¤ì‹œ í•œë²ˆ ë§í•´ì¤„ ìˆ˜ ìˆì„ê¹Œ?"

    # 2. context ìš”ì•½
    context_summary = "\n".join([f"- {chunk}" for chunk in retrieved_chunks])

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_builder = BasePromptBuilder(gender=gender, mode=mode, age=age, tf=tf)
    base_prompt = prompt_builder.build()

    system_prompt = f"""{base_prompt}

ì§€ê¸ˆë¶€í„°ëŠ” ì•„ë˜ ê³¼ê±° ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ìœ ì €ì˜ í˜„ì¬ ì§ˆë¬¸ì— ë” í’ë¶€í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•´ì¤˜:

{context_summary}
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ]

    # 4. ì‘ë‹µ ìƒì„±
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.7
        )
        reply = response.choices[0].message.content.strip()

        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(reply)

        return reply

    except Exception as e:
        print(f"[RAG ì‘ë‹µ ì‹¤íŒ¨] {e}")
        return "âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì¤˜!"
    
# âœ… ë²¡í„° ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ (vector vs memory)
def should_use_vector_search(user_input: str) -> bool:
    try:
        prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ê³¼ê±° ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•´ì•¼ í•  ë‚´ìš©ì¸ì§€ íŒë‹¨í•´ì¤˜.

ì§ˆë¬¸: "{user_input}"

ë‹µë³€ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ í•´ì¤˜:
- vector â†’ ê³¼ê±° ëŒ€í™” ê²€ìƒ‰ í•„ìš”
- memory â†’ í˜„ì¬ ëŒ€í™” íë¦„ì— ì‘ë‹µ

ì •ë‹µ:
"""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer = response.choices[0].message.content.strip().lower()
        return "vector" in answer or "ê³¼ê±°" in answer

    except Exception as e:
        print(f"[ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨] {e}")
        return False

